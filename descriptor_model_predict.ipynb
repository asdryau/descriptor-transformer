{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "descriptor_model_predict.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jv-Y9LAGwp-z"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buganart/descriptor-transformer/blob/main/descriptor_model_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbp-CL5ijb4e",
        "cellView": "form"
      },
      "source": [
        "#@markdown Before starting please save the notebook in your drive by clicking on `File -> Save a copy in drive`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ-pH7tyK9xW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "b07858ba-820a-4858-e33d-bfdc36241028"
      },
      "source": [
        "#@markdown Check GPU, should be a Tesla V100\n",
        "!nvidia-smi -L\n",
        "import os\n",
        "print(f\"We have {os.cpu_count()} CPU cores.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-5b4df872-b7b4-6bd6-6086-6c3f285303ba)\n",
            "We have 4 CPU cores.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJyxzcLOhgWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "ef8038f3-4bf1-4e38-8c6f-8e545e611483"
      },
      "source": [
        "#@markdown Mount google drive\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path(\"/content/drive/My Drive/IRCMS_GAN_collaborative_database\").exists():\n",
        "    raise RuntimeError(\n",
        "        \"Shortcut to our shared drive folder doesn't exits.\\n\\n\"\n",
        "        \"\\t1. Go to the google drive web UI\\n\"\n",
        "        \"\\t2. Right click shared folder IRCMS_GAN_collaborative_database and click \\\"Add shortcut to Drive\\\"\"\n",
        "    )\n",
        "\n",
        "def clear_on_success(msg=\"Ok!\"):\n",
        "    if _exit_code == 0:\n",
        "        output.clear()\n",
        "        print(msg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-L3BlfGTfbJ",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84b5b4a-0b3a-4565-b65a-13bb09afc900"
      },
      "source": [
        "#@markdown Install wandb and log in\n",
        "%pip install wandb\n",
        "output.clear()\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "wandb_drive_netrc_path = Path(\"drive/My Drive/colab/.netrc\")\n",
        "wandb_local_netrc_path = Path(\"/root/.netrc\")\n",
        "if wandb_drive_netrc_path.exists():\n",
        "    import shutil\n",
        "\n",
        "    print(\"Wandb .netrc file found, will use that to log in.\")\n",
        "    shutil.copy(wandb_drive_netrc_path, wandb_local_netrc_path)\n",
        "else:\n",
        "    print(\n",
        "        f\"Wandb config not found at {wandb_drive_netrc_path}.\\n\"\n",
        "        f\"Using manual login.\\n\\n\"\n",
        "        f\"To use auto login in the future, finish the manual login first and then run:\\n\\n\"\n",
        "        f\"\\t!mkdir -p '{wandb_drive_netrc_path.parent}'\\n\"\n",
        "        f\"\\t!cp {wandb_local_netrc_path} '{wandb_drive_netrc_path}'\\n\\n\"\n",
        "        f\"Then that file will be used to login next time.\\n\"\n",
        "    )\n",
        "\n",
        "!wandb login\n",
        "output.clear()\n",
        "print(\"ok!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ok!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVjGm8m_q9R6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf2eb51b-277a-4250-be51-9ad8118109bf"
      },
      "source": [
        "#@title Configuration\n",
        "\n",
        "#@markdown Directories can be found via file explorer on the left by navigating into `drive` to the desired folders. \n",
        "#@markdown Then right-click and `Copy path`.\n",
        "test_data_path = \"/content/drive/My Drive/AUDIO DATABASE/MUSIC TRANSFORMER/Transformer Corpus/\\u003CDirEntry \\\"Copy of 'Seven Words' for Violoncello, Bayan and String - VI. It is Finished!-RUzgq1MQA_g.wav\\\">.txt\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Resumption of previous runs\n",
        "#@markdown Optional resumption arguments below, leaving both empty will start a new run from scratch. \n",
        "#@markdown - The ID can be found on wandb. \n",
        "#@markdown - It's 8 characters long and may contain a-z letters and digits (for example `1t212ycn`).\n",
        "\n",
        "#@markdown Resume a previous run \n",
        "resume_run_id = \"4gn7g6xq\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown the number of predicted descriptors after the test_data\n",
        "prediction_length = 10 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown the path to save all generated descriptors as json\n",
        "prediction_output_dir = \"/content/drive/My Drive/Descriptor Model/OUTPUTS/\" #@param {type:\"string\"}\n",
        "\n",
        "import re\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "\n",
        "def check_wandb_id(run_id):\n",
        "    if run_id and not re.match(r\"^[\\da-z]{8}$\", run_id):\n",
        "        raise RuntimeError(\n",
        "            \"Run ID needs to be 8 characters long and contain only letters a-z and digits.\\n\"\n",
        "            f\"Got \\\"{run_id}\\\"\"\n",
        "        )\n",
        "\n",
        "check_wandb_id(resume_run_id)\n",
        "\n",
        "prediction_output_dir = Path(prediction_output_dir)\n",
        "prediction_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "colab_config = {\n",
        "    \"resume_run_id\": resume_run_id,\n",
        "    \"test_data_path\": test_data_path,\n",
        "    \"prediction_length\": prediction_length,\n",
        "    \"prediction_output_dir\": prediction_output_dir,\n",
        "}\n",
        "\n",
        "for k, v in colab_config.items():\n",
        "    print(f\"=> {k:20}: {v}\")\n",
        "\n",
        "config = Namespace(**colab_config)\n",
        "config.seed = 1234"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> resume_run_id       : 4gn7g6xq\n",
            "=> test_data_path      : /content/drive/My Drive/AUDIO DATABASE/MUSIC TRANSFORMER/Transformer Corpus/<DirEntry \"Copy of 'Seven Words' for Violoncello, Bayan and String - VI. It is Finished!-RUzgq1MQA_g.wav\">.txt\n",
            "=> prediction_length   : 10\n",
            "=> prediction_output_dir: /content/drive/My Drive/IRCMS_GAN_collaborative_database/Experiments/colab-violingan/descriptor-prediction-outputs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hCJPdJzKqCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04045f2e-f68b-4432-87f6-24899d45dd51"
      },
      "source": [
        "%pip install pytorch-lightning\r\n",
        "clear_on_success()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ok!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3knaZqbr7LC"
      },
      "source": [
        "#model\r\n",
        "only simple RNN is implemented.\r\n",
        "TODO: add more complicated time-series models (such as transformer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jLCYBBBi3Z1"
      },
      "source": [
        "import pytorch_lightning as pl\r\n",
        "from pytorch_lightning.callbacks.base import Callback\r\n",
        "from pytorch_lightning.loggers import WandbLogger\r\n",
        "import pprint\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "\r\n",
        "class DescriptorModel(pl.LightningModule):\r\n",
        "\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "        self.save_hyperparameters(\"config\")\r\n",
        "        descriptor_size = config.descriptor_size=5\r\n",
        "        hidden_size = config.hidden_size=100\r\n",
        "        num_layers = config.num_layers=3\r\n",
        "\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_layers = num_layers\r\n",
        "        self.lstm = nn.LSTM(descriptor_size, hidden_size, num_layers=num_layers, batch_first=True)\r\n",
        "        self.linear = nn.Linear(hidden_size, descriptor_size)\r\n",
        "        self.loss_function = nn.MSELoss()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        batch_size = x.shape[0]\r\n",
        "        h = (torch.zeros(self.num_layers, batch_size, self.hidden_size),\r\n",
        "            torch.zeros(self.num_layers, batch_size, self.hidden_size))\r\n",
        "        x, _ = self.lstm(x, h)\r\n",
        "        x = self.linear(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def training_step(self, batch, batch_idx):\r\n",
        "        data, target = batch\r\n",
        "        output = self(data)\r\n",
        "        pred = output[:,-1,:].unsqueeze(1)\r\n",
        "\r\n",
        "        loss = self.loss_function(pred, target)\r\n",
        "        return loss\r\n",
        "\r\n",
        "    def configure_optimizers(self):\r\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.config.learning_rate)\r\n",
        "\r\n",
        "    def predict(self, data, step):\r\n",
        "        all_descriptors = data\r\n",
        "        batch_size, window_size, des_size = data.shape\r\n",
        "        for i in range(step):\r\n",
        "            input = all_descriptors[:,i:,:]\r\n",
        "            # print(\"input\", input)\r\n",
        "            pred = self(input)\r\n",
        "            new_descriptor = pred[:,1,:].reshape(batch_size, 1, des_size)\r\n",
        "            # print(\"new_descriptor\", new_descriptor)\r\n",
        "            all_descriptors = torch.cat((all_descriptors, new_descriptor), 1)\r\n",
        "        return all_descriptors.detach().cpu().numpy()[:,-step:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv-Y9LAGwp-z"
      },
      "source": [
        "#helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeKrMjUtwqK6"
      },
      "source": [
        "def load_checkpoint_from_cloud(checkpoint_path=\"model_dict.pth\"):\r\n",
        "    checkpoint_file = wandb.restore(checkpoint_path)\r\n",
        "    return checkpoint_file.name\r\n",
        "\r\n",
        "def get_resume_run_config(resume_id):\r\n",
        "    # all config will be replaced by the stored one in wandb\r\n",
        "    api = wandb.Api()\r\n",
        "    previous_run = api.run(f\"demiurge/descriptor_model/{resume_id}\")\r\n",
        "    config = Namespace(**previous_run.config)\r\n",
        "    return config\r\n",
        "\r\n",
        "def load_test_data(config, test_data_path):\r\n",
        "    test_data_path = Path(test_data_path)\r\n",
        "    window_size = config.window_size\r\n",
        "\r\n",
        "    attribute_list = []\r\n",
        "    des_array = None\r\n",
        "\r\n",
        "    with open(test_data_path) as json_file:\r\n",
        "        data = json.load(json_file)\r\n",
        "        data_list = []\r\n",
        "        for des in data:\r\n",
        "            timestamp = next(iter(des))\r\n",
        "            descriptor = des[timestamp]\r\n",
        "            if len(attribute_list) == 0:\r\n",
        "                attribute_list = descriptor.keys()\r\n",
        "                attribute_list = sorted(attribute_list)\r\n",
        "            values = []\r\n",
        "            for k in attribute_list:\r\n",
        "                values.append(float(descriptor[k]))\r\n",
        "            data_list.append((int(timestamp), values))\r\n",
        "        #sort value by timestamp\r\n",
        "        sorted_data = sorted(data_list)\r\n",
        "        #convert data into descriptor array\r\n",
        "        des_array = [j for (i,j) in sorted_data]\r\n",
        "    des_array = np.array(des_array)\r\n",
        "    #   cut according to the window size\r\n",
        "    des_array = des_array[np.newaxis, -window_size:, :]\r\n",
        "    #   also need to record attribute_list and timeframe for saving\r\n",
        "    last_timestamp = sorted_data[-1][0]\r\n",
        "    interval = sorted_data[-1][0] - sorted_data[-2][0]\r\n",
        "    return torch.tensor(des_array, dtype=torch.float32), (int(last_timestamp), int(interval), attribute_list)\r\n",
        "\r\n",
        "def save_descriptor_as_json(save_path, data, audio_info):\r\n",
        "    last_timestamp, interval, attribute_list = audio_info\r\n",
        "    current_timestamp = last_timestamp + interval\r\n",
        "    num_data, prediction_length, _ = data.shape\r\n",
        "    for i in range(num_data):\r\n",
        "        #for each data, save 1 json file\r\n",
        "        stored = []\r\n",
        "        for j in range(prediction_length):\r\n",
        "            descriptor_info = {}\r\n",
        "            descriptor_values = {}\r\n",
        "            for k in range(len(attribute_list)):\r\n",
        "                descriptor_values[str(attribute_list[k])] = str(data[i,j,k])\r\n",
        "            descriptor_info[str(current_timestamp)] = descriptor_values\r\n",
        "            stored.append(descriptor_info)\r\n",
        "            #increment timestamp for next descriptor\r\n",
        "            current_timestamp = current_timestamp + interval\r\n",
        "\r\n",
        "        #save to json\r\n",
        "        json_path = save_path / (str(resume_run_id) + \"_predicted_\" + str(i) + \".txt\")\r\n",
        "        with open(json_path, \"w\") as json_file:\r\n",
        "            json.dump(stored, json_file)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#######################         train functions\r\n",
        "\r\n",
        "\r\n",
        "def init_wandb_run(config, run_dir=\"./\", mode=\"run\"):\r\n",
        "    resume_run_id = config.resume_run_id\r\n",
        "    entity = \"demiurge\"\r\n",
        "    run_dir = Path(run_dir).absolute()\r\n",
        "\r\n",
        "    if resume_run_id:\r\n",
        "        run_id = resume_run_id\r\n",
        "    else:\r\n",
        "        run_id = wandb.util.generate_id()\r\n",
        "\r\n",
        "    run = wandb.init(\r\n",
        "        project=\"descriptor_model\",\r\n",
        "        id=run_id,\r\n",
        "        entity=entity,\r\n",
        "        resume=True,\r\n",
        "        dir=run_dir,\r\n",
        "        mode=mode,\r\n",
        "    )\r\n",
        "\r\n",
        "    print(\"run id: \" + str(wandb.run.id))\r\n",
        "    print(\"run name: \" + str(wandb.run.name))\r\n",
        "    wandb.watch_called = False\r\n",
        "    # run.tags = run.tags + (selected_model,)\r\n",
        "    return run\r\n",
        "\r\n",
        "def setup_model(config, run):\r\n",
        "    checkpoint_path = str(Path(run.dir).absolute() / \"checkpoint.ckpt\")\r\n",
        "\r\n",
        "    if config.resume_run_id:\r\n",
        "        # Download file from the wandb cloud.\r\n",
        "        load_checkpoint_from_cloud(checkpoint_path=\"checkpoint.ckpt\")\r\n",
        "        extra_trainer_args = {\"resume_from_checkpoint\": checkpoint_path}\r\n",
        "        model = DescriptorModel.load_from_checkpoint(checkpoint_path)\r\n",
        "    else:\r\n",
        "        extra_trainer_args = {}\r\n",
        "        model = DescriptorModel(config)\r\n",
        "\r\n",
        "    return model, extra_trainer_args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liDBc0QQFtuM"
      },
      "source": [
        "# generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX-QEhDcFt3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0e39547-06ab-4be3-ca37-e1607a984166"
      },
      "source": [
        "config = get_resume_run_config(resume_run_id)\r\n",
        "config.resume_run_id = resume_run_id\r\n",
        "run = init_wandb_run(config, run_dir=\"./\", mode=\"offline\")\r\n",
        "model,_ = setup_model(config, run)\r\n",
        "model.eval()\r\n",
        "#construct test_data\r\n",
        "test_data, audio_info = load_test_data(config, test_data_path)\r\n",
        "\r\n",
        "prediction = model.predict(test_data, prediction_length)\r\n",
        "\r\n",
        "save_descriptor_as_json(prediction_output_dir, prediction, audio_info)\r\n",
        "print(\"ok!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Offline run mode, not syncing to the cloud.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to `offline` in this directory.  Run `wandb online` to enable cloud syncing.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "run id: 4gn7g6xq\n",
            "run name: None\n",
            "ok!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}