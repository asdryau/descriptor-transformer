{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "descriptor_model_train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4qfTNGu-Ip2D",
        "h3knaZqbr7LC",
        "jv-Y9LAGwp-z"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buganart/descriptor-transformer/blob/main/descriptor_model_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbp-CL5ijb4e"
      },
      "source": [
        "#@markdown Before starting please save the notebook in your drive by clicking on `File -> Save a copy in drive`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ-pH7tyK9xW"
      },
      "source": [
        "#@markdown Check GPU, should be a Tesla V100\n",
        "!nvidia-smi -L\n",
        "import os\n",
        "print(f\"We have {os.cpu_count()} CPU cores.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJyxzcLOhgWY",
        "cellView": "form"
      },
      "source": [
        "#@markdown Mount google drive\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path(\"/content/drive/My Drive/IRCMS_GAN_collaborative_database\").exists():\n",
        "    raise RuntimeError(\n",
        "        \"Shortcut to our shared drive folder doesn't exits.\\n\\n\"\n",
        "        \"\\t1. Go to the google drive web UI\\n\"\n",
        "        \"\\t2. Right click shared folder IRCMS_GAN_collaborative_database and click \\\"Add shortcut to Drive\\\"\"\n",
        "    )\n",
        "\n",
        "def clear_on_success(msg=\"Ok!\"):\n",
        "    if _exit_code == 0:\n",
        "        output.clear()\n",
        "        print(msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-L3BlfGTfbJ",
        "cellView": "form"
      },
      "source": [
        "#@markdown Install wandb and log in\n",
        "%pip install wandb\n",
        "output.clear()\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "wandb_drive_netrc_path = Path(\"drive/My Drive/colab/.netrc\")\n",
        "wandb_local_netrc_path = Path(\"/root/.netrc\")\n",
        "if wandb_drive_netrc_path.exists():\n",
        "    import shutil\n",
        "\n",
        "    print(\"Wandb .netrc file found, will use that to log in.\")\n",
        "    shutil.copy(wandb_drive_netrc_path, wandb_local_netrc_path)\n",
        "else:\n",
        "    print(\n",
        "        f\"Wandb config not found at {wandb_drive_netrc_path}.\\n\"\n",
        "        f\"Using manual login.\\n\\n\"\n",
        "        f\"To use auto login in the future, finish the manual login first and then run:\\n\\n\"\n",
        "        f\"\\t!mkdir -p '{wandb_drive_netrc_path.parent}'\\n\"\n",
        "        f\"\\t!cp {wandb_local_netrc_path} '{wandb_drive_netrc_path}'\\n\\n\"\n",
        "        f\"Then that file will be used to login next time.\\n\"\n",
        "    )\n",
        "\n",
        "!wandb login\n",
        "output.clear()\n",
        "print(\"ok!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVjGm8m_q9R6"
      },
      "source": [
        "#@title Configuration\n",
        "\n",
        "#@markdown Directories can be found via file explorer on the left by navigating into `drive` to the desired folders. \n",
        "#@markdown Then right-click and `Copy path`.\n",
        "audio_db_dir = \"/content/drive/My Drive/AUDIO DATABASE/MUSIC TRANSFORMER/Transformer Corpus\" #@param {type:\"string\"}\n",
        "# audio_db_dir = \"/content/drive/My Drive/AUDIO DATABASE/TESTING\" #@param {type:\"string\"}\n",
        "experiment_dir = \"/content/drive/My Drive/IRCMS_GAN_collaborative_database/Experiments/colab-violingan/descriptor-model\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Resumption of previous runs\n",
        "#@markdown Optional resumption arguments below, leaving both empty will start a new run from scratch. \n",
        "#@markdown - The ID can be found on wandb. \n",
        "#@markdown - It's 8 characters long and may contain a-z letters and digits (for example `1t212ycn`).\n",
        "\n",
        "#@markdown Resume a previous run \n",
        "resume_run_id = \"4gn7g6xq\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown train argument\n",
        "window_size = 15 #@param {type: \"integer\"}\n",
        "learning_rate = 1e-4 #@param {type: \"number\"}\n",
        "batch_size = 64 #@param {type: \"integer\"}\n",
        "epochs = 3000 #@param {type: \"integer\"}\n",
        "\n",
        "# log_interval = 10 #@param {type: \"integer\"}\n",
        "save_interval = 10 #@param {type: \"integer\"}\n",
        "# n_test_samples = 8 #@param {type: \"integer\"}\n",
        "\n",
        "notes = \"\" #@param {type: \"string\"}\n",
        "\n",
        "import re\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "\n",
        "audio_db_dir = Path(audio_db_dir)\n",
        "experiment_dir = Path(experiment_dir)\n",
        "\n",
        "\n",
        "for path in [experiment_dir]:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not audio_db_dir.exists():\n",
        "    raise RuntimeError(f\"audio_db_dir {audio_db_dir} does not exists.\")\n",
        "\n",
        "def check_wandb_id(run_id):\n",
        "    if run_id and not re.match(r\"^[\\da-z]{8}$\", run_id):\n",
        "        raise RuntimeError(\n",
        "            \"Run ID needs to be 8 characters long and contain only letters a-z and digits.\\n\"\n",
        "            f\"Got \\\"{run_id}\\\"\"\n",
        "        )\n",
        "\n",
        "check_wandb_id(resume_run_id)\n",
        "\n",
        "colab_config = {\n",
        "    \"audio_db_dir\": audio_db_dir,\n",
        "    \"experiment_dir\": experiment_dir,\n",
        "    \"resume_run_id\": resume_run_id,\n",
        "    \"window_size\": window_size,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"epochs\": epochs,\n",
        "    \"save_interval\": save_interval,\n",
        "    \"notes\": notes,\n",
        "}\n",
        "\n",
        "for k, v in colab_config.items():\n",
        "    print(f\"=> {k:20}: {v}\")\n",
        "\n",
        "config = Namespace(**colab_config)\n",
        "config.seed = 1234\n",
        "\n",
        "config.descriptor_size=5\n",
        "config.hidden_size=100\n",
        "config.num_layers=3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hCJPdJzKqCW"
      },
      "source": [
        "%pip install pytorch-lightning\r\n",
        "clear_on_success()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qfTNGu-Ip2D"
      },
      "source": [
        "#load descriptor files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cx8DDqpIp-K"
      },
      "source": [
        "import json\r\n",
        "import tqdm\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch.utils.data import DataLoader, TensorDataset\r\n",
        "import torch.nn as nn\r\n",
        "import pytorch_lightning as pl\r\n",
        "\r\n",
        "from pytorch_lightning.callbacks.base import Callback\r\n",
        "from pytorch_lightning.loggers import WandbLogger\r\n",
        "import pprint\r\n",
        "\r\n",
        "class DataModule_descriptor(pl.LightningDataModule):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.data_path = config.audio_db_dir\r\n",
        "        self.attribute_list = None\r\n",
        "        self.dataset_input = None\r\n",
        "        self.dataset_target = None\r\n",
        "        self.window_size = config.window_size\r\n",
        "        self.batch_size = config.batch_size\r\n",
        "        \r\n",
        "    # def prepare_data(self):\r\n",
        "    #     pass\r\n",
        "    def setup(self, stage=None):\r\n",
        "        window_size = self.window_size\r\n",
        "        filepath_list = self.data_path.rglob(\"*.*\")\r\n",
        "        # check files in filepath_list is supported (by extensions)\r\n",
        "        filepath_list = [\r\n",
        "            path\r\n",
        "            for path in filepath_list\r\n",
        "            if Path(path).suffix == \".txt\"\r\n",
        "        ]\r\n",
        "\r\n",
        "        attribute_list = []\r\n",
        "        dataset_input = []\r\n",
        "        dataset_target = []\r\n",
        "        # process files in the filepath_list\r\n",
        "        for path in tqdm.tqdm(filepath_list, desc=\"Descriptor Files\"):\r\n",
        "            \r\n",
        "            with open(path) as json_file:\r\n",
        "                data = json.load(json_file)\r\n",
        "                data_list = []\r\n",
        "                for des in data:\r\n",
        "                    timestamp = next(iter(des))\r\n",
        "                    descriptor = des[timestamp]\r\n",
        "                    if len(attribute_list) == 0:\r\n",
        "                        attribute_list = descriptor.keys()\r\n",
        "                        attribute_list = sorted(attribute_list)\r\n",
        "                    values = []\r\n",
        "                    for k in attribute_list:\r\n",
        "                        values.append(float(descriptor[k]))\r\n",
        "                    data_list.append((int(timestamp), values))\r\n",
        "                #sort value by timestamp\r\n",
        "                sorted_data = sorted(data_list)\r\n",
        "                #convert data into descriptor array\r\n",
        "                des_array = [j for (i,j) in sorted_data]\r\n",
        "                des_array = np.array(des_array)\r\n",
        "                num_des = des_array.shape[0]\r\n",
        "                #pack descriptors into batches based on window_size\r\n",
        "                input_array = []\r\n",
        "                target_array = []\r\n",
        "                for i in range(num_des - window_size - 1):\r\n",
        "                    input_batch = des_array[i:i+window_size]\r\n",
        "                    target_batch = des_array[i+1+window_size]\r\n",
        "                    target_batch = target_batch[np.newaxis,:]\r\n",
        "                    input_array.append(input_batch)\r\n",
        "                    target_array.append(target_batch)\r\n",
        "\r\n",
        "                #add processed array to dataset\r\n",
        "                dataset_input.append(input_array)\r\n",
        "                dataset_target.append(target_array)\r\n",
        "\r\n",
        "        self.attribute_list = attribute_list\r\n",
        "        self.dataset_input = np.concatenate(dataset_input, axis=0)\r\n",
        "        self.dataset_target = np.concatenate(dataset_target, axis=0)\r\n",
        "    def train_dataloader(self):\r\n",
        "        batch_size = self.batch_size\r\n",
        "        dataset = TensorDataset(torch.tensor(self.dataset_input, dtype=torch.float32), torch.tensor(self.dataset_target, dtype=torch.float32))\r\n",
        "        dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=True,num_workers=8)\r\n",
        "        return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3knaZqbr7LC"
      },
      "source": [
        "#model\r\n",
        "only simple RNN is implemented.\r\n",
        "TODO: add more complicated time-series models (such as transformer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jLCYBBBi3Z1"
      },
      "source": [
        "class DescriptorModel(pl.LightningModule):\r\n",
        "\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "        self.save_hyperparameters(\"config\")\r\n",
        "        descriptor_size = config.descriptor_size=5\r\n",
        "        hidden_size = config.hidden_size=100\r\n",
        "        num_layers = config.num_layers=3\r\n",
        "\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_layers = num_layers\r\n",
        "        self.lstm = nn.LSTM(descriptor_size, hidden_size, num_layers=num_layers, batch_first=True)\r\n",
        "        self.linear = nn.Linear(hidden_size, descriptor_size)\r\n",
        "        self.loss_function = nn.MSELoss()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        batch_size = x.shape[0]\r\n",
        "        h = (torch.zeros(self.num_layers, batch_size, self.hidden_size),\r\n",
        "            torch.zeros(self.num_layers, batch_size, self.hidden_size))\r\n",
        "        x, _ = self.lstm(x, h)\r\n",
        "        x = self.linear(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def training_step(self, batch, batch_idx):\r\n",
        "        data, target = batch\r\n",
        "        output = self(data)\r\n",
        "        pred = output[:,-1,:].unsqueeze(1)\r\n",
        "\r\n",
        "        loss = self.loss_function(pred, target)\r\n",
        "        return loss\r\n",
        "\r\n",
        "    def configure_optimizers(self):\r\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.config.learning_rate)\r\n",
        "\r\n",
        "    def predict(self, data, step):\r\n",
        "        all_descriptors = data\r\n",
        "        batch_size, window_size, des_size = data.shape\r\n",
        "        for i in range(step):\r\n",
        "            input = all_descriptors[:,i:,:]\r\n",
        "            # print(\"input\", input)\r\n",
        "            pred = self(input)\r\n",
        "            new_descriptor = pred[:,1,:].reshape(batch_size, 1, des_size)\r\n",
        "            # print(\"new_descriptor\", new_descriptor)\r\n",
        "            all_descriptors = torch.cat((all_descriptors, new_descriptor), 1)\r\n",
        "        return all_descriptors.detach().cpu().numpy()[:,-step:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv-Y9LAGwp-z"
      },
      "source": [
        "#helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeKrMjUtwqK6"
      },
      "source": [
        "class SaveWandbCallback(Callback):\r\n",
        "    def __init__(self, log_interval, save_model_path):\r\n",
        "        super().__init__()\r\n",
        "        self.epoch = 0\r\n",
        "        self.log_interval = log_interval\r\n",
        "        self.save_model_path = save_model_path\r\n",
        "\r\n",
        "    def on_train_epoch_end(self, trainer, pl_module, outputs):\r\n",
        "        if self.epoch % self.log_interval == 0:\r\n",
        "            # log\r\n",
        "            trainer.save_checkpoint(self.save_model_path)\r\n",
        "            save_checkpoint_to_cloud(self.save_model_path)\r\n",
        "        self.epoch += 1\r\n",
        "\r\n",
        "# function to save/load files from wandb\r\n",
        "def save_checkpoint_to_cloud(checkpoint_path):\r\n",
        "    wandb.save(checkpoint_path)\r\n",
        "\r\n",
        "\r\n",
        "def load_checkpoint_from_cloud(checkpoint_path=\"model_dict.pth\"):\r\n",
        "    checkpoint_file = wandb.restore(checkpoint_path)\r\n",
        "    return checkpoint_file.name\r\n",
        "\r\n",
        "#######################         train functions\r\n",
        "\r\n",
        "def save_model_args(config, run):\r\n",
        "    filepath = str(Path(run.dir).absolute() / \"model_args.json\")\r\n",
        "\r\n",
        "    config = vars(config)\r\n",
        "    config_dict = {}\r\n",
        "    for k in config.keys():\r\n",
        "        config_dict[k] = str(config[k])\r\n",
        "    with open(filepath, \"w\") as fp:\r\n",
        "        json.dump(config_dict, fp)\r\n",
        "    save_checkpoint_to_cloud(filepath)\r\n",
        "\r\n",
        "def init_wandb_run(config, run_dir=\"./\", mode=\"run\"):\r\n",
        "    resume_run_id = config.resume_run_id\r\n",
        "    entity = \"demiurge\"\r\n",
        "    run_dir = Path(run_dir).absolute()\r\n",
        "\r\n",
        "    if resume_run_id:\r\n",
        "        run_id = resume_run_id\r\n",
        "    else:\r\n",
        "        run_id = wandb.util.generate_id()\r\n",
        "\r\n",
        "    run = wandb.init(\r\n",
        "        project=\"descriptor_model\",\r\n",
        "        id=run_id,\r\n",
        "        entity=entity,\r\n",
        "        resume=True,\r\n",
        "        dir=run_dir,\r\n",
        "        mode=mode,\r\n",
        "    )\r\n",
        "\r\n",
        "    print(\"run id: \" + str(wandb.run.id))\r\n",
        "    print(\"run name: \" + str(wandb.run.name))\r\n",
        "    wandb.watch_called = False\r\n",
        "    # run.tags = run.tags + (selected_model,)\r\n",
        "    return run\r\n",
        "\r\n",
        "def setup_datamodule(config):\r\n",
        "    np.random.seed(config.seed)\r\n",
        "    torch.manual_seed(config.seed)\r\n",
        "\r\n",
        "    dataModule = DataModule_descriptor(config)\r\n",
        "    return dataModule\r\n",
        "\r\n",
        "def setup_model(config, run):\r\n",
        "    checkpoint_path = str(Path(run.dir).absolute() / \"checkpoint.ckpt\")\r\n",
        "\r\n",
        "    if config.resume_run_id:\r\n",
        "        # Download file from the wandb cloud.\r\n",
        "        load_checkpoint_from_cloud(checkpoint_path=\"checkpoint.ckpt\")\r\n",
        "        extra_trainer_args = {\"resume_from_checkpoint\": checkpoint_path}\r\n",
        "        model = DescriptorModel.load_from_checkpoint(checkpoint_path)\r\n",
        "    else:\r\n",
        "        extra_trainer_args = {}\r\n",
        "        model = DescriptorModel(config)\r\n",
        "\r\n",
        "    return model, extra_trainer_args\r\n",
        "\r\n",
        "def train(config, run, model, dataModule, extra_trainer_args):\r\n",
        "    np.random.seed(config.seed)\r\n",
        "    torch.manual_seed(config.seed)\r\n",
        "\r\n",
        "    # wandb logger setup\r\n",
        "    wandb_logger = WandbLogger(\r\n",
        "        experiment=run, log_model=True, save_dir=Path(run.dir).absolute()\r\n",
        "    )\r\n",
        "\r\n",
        "    # log config\r\n",
        "    wandb.config.update(config)\r\n",
        "    save_model_args(config, run)\r\n",
        "    pprint.pprint(vars(config))\r\n",
        "\r\n",
        "    checkpoint_path = str(Path(run.dir).absolute() / \"checkpoint.ckpt\")\r\n",
        "    callbacks = [SaveWandbCallback(config.save_interval, checkpoint_path)]\r\n",
        "\r\n",
        "    trainer = pl.Trainer(\r\n",
        "        max_epochs=config.epochs,\r\n",
        "        logger=wandb_logger,\r\n",
        "        callbacks=callbacks,\r\n",
        "        default_root_dir=wandb.run.dir,\r\n",
        "        checkpoint_callback=None,\r\n",
        "        **extra_trainer_args,\r\n",
        "    )\r\n",
        "\r\n",
        "    # train\r\n",
        "    trainer.fit(model, dataModule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5w7a7NBws3i"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAlQ8vbQIvnE"
      },
      "source": [
        "run = init_wandb_run(config, run_dir=experiment_dir)#, mode=\"offline\")\r\n",
        "datamodule = setup_datamodule(config)\r\n",
        "model, extra_trainer_args = setup_model(config, run)\r\n",
        "# model = DescriptorModel(config)\r\n",
        "extra_trainer_args = {}\r\n",
        "train(config, run, model, datamodule, extra_trainer_args)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}